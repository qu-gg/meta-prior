{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@file metaprior_analysis.ipynb\n",
    "@author Ryan Missel\n",
    "\n",
    "This file is to deep dive into the code written for the MetaPrior and making sure that each component\n",
    "is functioning as it should, specifically how the codes are sampled and built at each iteration.\n",
    "\n",
    "In non-binary classification tasks, the model is having troubles converging - lending to the idea that\n",
    "perhaps the code buildup is incorrect. The classifier often first converges at linear decision boundaries\n",
    "through the classes and cannot converge nicely to a cross in four classes. Convergence for the 3 class problem\n",
    "takes many iterations.\n",
    "\"\"\"\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import entropy\n",
    "from metaprior.metautils import get_act, plot_metaspace_mean, plot_metaspace_var, plot_weight_correlations\n",
    "from torch.distributions import Normal, kl_divergence as kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaPrior(nn.Module):\n",
    "    def __init__(self, layer_sizes=[1, 100, 1], mean=False, local=False, activation='linear', hyperprior_dim=32, code_dim=2):\n",
    "        super(MetaPrior, self).__init__()\n",
    "        self.input_dim = layer_sizes[0]\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.code_dim = code_dim\n",
    "\n",
    "        self.local = local\n",
    "        self.mean = mean\n",
    "\n",
    "        self.acts = [get_act('leaky_relu') if i < len(layer_sizes) - 2 else get_act(activation)\n",
    "                         for i in range(len(layer_sizes) - 1)]\n",
    "\n",
    "        self.embedder = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 20),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(20, code_dim)\n",
    "        )\n",
    "        \n",
    "        self.weight_codes = []\n",
    "        self.bias_codes = []\n",
    "\n",
    "        # Define the initial weight code distribution parameters\n",
    "        self.code_mu = nn.ParameterList([\n",
    "            torch.nn.Parameter(\n",
    "                torch.zeros([lsize, code_dim]), #+ 0.1 * torch.randn([lsize, code_dim], requires_grad=True),\n",
    "                requires_grad=True)\n",
    "            for lsize in self.layer_sizes\n",
    "        ])\n",
    "\n",
    "        self.code_var = nn.ParameterList([\n",
    "            torch.nn.Parameter(torch.ones([lsize, code_dim]), # + 0.1 * torch.randn([lsize, code_dim], requires_grad=True),\n",
    "                               requires_grad=True)\n",
    "            for lsize in self.layer_sizes\n",
    "        ])\n",
    "\n",
    "        self.codes = [\n",
    "            torch.randn([lsize, code_dim]) for lsize in self.layer_sizes\n",
    "        ]\n",
    "\n",
    "\n",
    "        # Define the hyperprior network that generates the distribution parameters of the Weights\n",
    "        self.hyperprior = nn.Sequential(\n",
    "            nn.Linear(code_dim * 2, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, hyperprior_dim)\n",
    "        )\n",
    "\n",
    "        self.mean_net = nn.Linear(hyperprior_dim, 1)\n",
    "        self.var_net = nn.Linear(hyperprior_dim, 1)\n",
    "\n",
    "    def generate_weight_codes(self):\n",
    "        \"\"\"\n",
    "        Handles building the weight codes and draw samples from i\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Sample the codes array\n",
    "        self.codes = [\n",
    "            self.code_mu[i] + torch.randn_like(self.code_mu[i]) * self.code_var[i]\n",
    "            for i in range(len(self.layer_sizes))\n",
    "        ]\n",
    "\n",
    "        self.weight_codes = []\n",
    "        self.bias_codes = []\n",
    "\n",
    "        # Loop between the layers and generate their weight codes by concatenating each units' latent var.\n",
    "        # Units in the smaller layer need to be duplicated to the size of the next layer in order to perform\n",
    "        # easy concatenation between their latent variables\n",
    "        for idx in range(len(self.layer_sizes) - 1):\n",
    "            temp = self.codes[idx].unsqueeze(1).repeat(1, self.layer_sizes[idx + 1], 1).view([-1, self.code_dim])\n",
    "            temp2 = self.codes[idx + 1].unsqueeze(0).repeat(self.layer_sizes[idx], 1, 1).view([-1, self.code_dim])\n",
    "            concated = torch.cat((temp, temp2), dim=1)\n",
    "\n",
    "            self.weight_codes.append(concated)\n",
    "\n",
    "            # Generate bias codes (concatenation is just with a zeros vector)\n",
    "            self.bias_codes.append(torch.cat((self.codes[idx + 1], torch.zeros_like(self.codes[idx + 1])), dim=1))\n",
    "\n",
    "    def kl_z_term(self):\n",
    "        \"\"\"\n",
    "        KL term related to the distribution parameters of the meta-variables, with prior N(0, 1)\n",
    "        :return: Sum of the KL values over each latent variable\n",
    "        \"\"\"\n",
    "        mus = torch.cat([cmu.view([-1]) for cmu in self.code_mu])\n",
    "        var = torch.cat([cvar.view([-1]) for cvar in self.code_var])\n",
    "\n",
    "        q = Normal(mus, var)\n",
    "        N = Normal(torch.zeros(len(mus), device=mus.device), torch.ones(len(mus), device=mus.device))\n",
    "        klz = kl(q, N).sum()\n",
    "        return klz\n",
    "\n",
    "    def forward(self, x, perturb=False):\n",
    "        \"\"\"\n",
    "        Handles iterating through each layer, generating the distribution parameters and sampling\n",
    "        the weights and biases for that layer\n",
    "        :param x: input x\n",
    "        :param perturb: whether to perturb one meta-var\n",
    "        \"\"\"\n",
    "        # Generate weight codes given current latent codes\n",
    "        self.generate_weight_codes()\n",
    "        local_code = self.embedder(x)\n",
    "\n",
    "        # Perturb one meta-var to test function draws\n",
    "        if perturb:\n",
    "            indice = np.random.randint(0, self.weight_codes[0].shape[0], 1)\n",
    "            self.weight_codes[0][indice] += torch.hstack((torch.zeros([2]), 2 * torch.randn([2])))\n",
    "\n",
    "        # Iterate over layers to get output\n",
    "        for lidx in range(len(self.layer_sizes) - 1):\n",
    "            # Get latent codes of hyperprior\n",
    "            if self.local:\n",
    "                latent_w = self.hyperprior(torch.cat((self.weight_codes[lidx], local_code), dim=1))\n",
    "                latent_b = self.hyperprior(torch.cat((self.bias_codes[lidx], local_code), dim=1))\n",
    "            else:\n",
    "                latent_w = self.hyperprior(self.weight_codes[lidx])\n",
    "                latent_b = self.hyperprior(self.bias_codes[lidx])\n",
    "\n",
    "            # Sample weights\n",
    "            w_mu, w_var = self.mean_net(latent_w), self.var_net(latent_w)\n",
    "            w = (w_mu + torch.randn_like(w_mu) * w_var).view([self.layer_sizes[lidx], self.layer_sizes[lidx + 1]])\n",
    "\n",
    "            # Sample biases\n",
    "            b_mu, b_var = self.mean_net(latent_b), self.var_net(latent_b)\n",
    "            b = (b_mu + torch.randn_like(b_mu) * b_var).squeeze()\n",
    "            \n",
    "            print(b.shape)\n",
    "\n",
    "            # Perform the linear layer and activate\n",
    "            x = func.linear(x, w.T, b)\n",
    "            x = self.acts[lidx](x)\n",
    "            \n",
    "        return x\n",
    "\n",
    "    def bce_predict(self, x):\n",
    "        \"\"\" Simply turns the softmax outputs into full class predictions \"\"\"\n",
    "        # Apply softmax to output.\n",
    "        pred = self.forward(x)\n",
    "\n",
    "        ans = []\n",
    "        # Pick the class with maximum weight\n",
    "        for t in pred:\n",
    "            if t < 0.5:\n",
    "                ans.append(0)\n",
    "            else:\n",
    "                ans.append(1)\n",
    "        return ans\n",
    "\n",
    "    def ce_predict(self, x):\n",
    "        \"\"\" Get class predictions from prediction vector \"\"\"\n",
    "        pred = self.forward(x).detach().cpu().numpy()\n",
    "        argmax = np.argmax(pred, axis=1)\n",
    "        return argmax\n",
    "\n",
    "    def weight_correlations(self, x, indice, layer, shift):\n",
    "        \"\"\" Get the weight vectors of two random nodes to another \"\"\"\n",
    "        # Clamp the weight code to its distribution mean\n",
    "        # self.codes = [self.code_mu[i] for i in range(len(self.layer_sizes))]\n",
    "        self.codes = [\n",
    "            self.code_mu[i] + torch.randn_like(self.code_mu[i]) * self.code_var[i]\n",
    "            for i in range(len(self.layer_sizes))\n",
    "        ]\n",
    "\n",
    "        self.codes[layer + 1][indice] = self.code_mu[layer + 1][indice] + torch.Tensor(shift)\n",
    "\n",
    "        # Generate weight codes\n",
    "        self.weight_codes = []\n",
    "        self.bias_codes = []\n",
    "\n",
    "        # Loop between the layers and generate their weight codes by concatenating each units' latent var.\n",
    "        # Units in the smaller layer need to be duplicated to the size of the next layer in order to perform\n",
    "        # easy concatenation between their latent variables\n",
    "        for idx in range(len(self.layer_sizes) - 1):\n",
    "            temp = self.codes[idx].unsqueeze(1).repeat(1, self.layer_sizes[idx + 1], 1).view([-1, self.code_dim])\n",
    "            temp2 = self.codes[idx + 1].unsqueeze(0).repeat(self.layer_sizes[idx], 1, 1).view([-1, self.code_dim])\n",
    "            concated = torch.cat((temp, temp2), dim=1)\n",
    "\n",
    "            self.weight_codes.append(concated)\n",
    "\n",
    "            # Generate bias codes (concatenation is just with a zeros vector)\n",
    "            self.bias_codes.append(torch.cat((self.codes[idx + 1], torch.zeros_like(self.codes[idx + 1])), dim=1))\n",
    "\n",
    "        # Iterate over layers to get output\n",
    "        w_out = None\n",
    "        for lidx in range(len(self.layer_sizes) - 1):\n",
    "            # Get latent codes of hyperprior\n",
    "            latent_w = self.hyperprior(self.weight_codes[lidx])\n",
    "            latent_b = self.hyperprior(self.bias_codes[lidx])\n",
    "\n",
    "            # Sample weights\n",
    "            w_mu, w_var = self.mean_net(latent_w), self.var_net(latent_w)\n",
    "            w = (w_mu + torch.randn_like(w_mu) * w_var.exp()).view([self.layer_sizes[lidx], self.layer_sizes[lidx + 1]])\n",
    "\n",
    "            # Get the sampled weight for the specific layer\n",
    "            if lidx == layer:\n",
    "                w_out = w\n",
    "\n",
    "            # Sample biases\n",
    "            b_mu, b_var = self.mean_net(latent_b), self.var_net(latent_b)\n",
    "            b = (b_mu + torch.randn_like(b_mu) * b_var.exp()).squeeze()\n",
    "\n",
    "            # Perform the linear layer and activate\n",
    "            x = func.linear(x, w.T, b)\n",
    "            x = self.acts[lidx](x)\n",
    "\n",
    "        # Return only the node weights that are cared about\n",
    "        return x, w_out[:, indice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MetaPrior(layer_sizes=[1, 100, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2764,  1.3279,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.2764,  1.3279, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.2764,  1.3279, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.3536, -0.0839,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.3536, -0.0839, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.3536, -0.0839, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.3103,  0.6195,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.3103,  0.6195, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.3103,  0.6195, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.0772, -0.7354,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.0772, -0.7354, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.0772, -0.7354, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.7057, -2.3433,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.7057, -2.3433, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.7057, -2.3433, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-2.7027,  0.9309,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-2.7027,  0.9309, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-2.7027,  0.9309, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.2554, -1.4502,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.2554, -1.4502, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.2554, -1.4502, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.9422, -0.4764,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.9422, -0.4764, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.9422, -0.4764, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.8576, -1.6589,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.8576, -1.6589, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.8576, -1.6589, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.5783, -1.6524,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.5783, -1.6524, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.5783, -1.6524, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.1412, -1.7758,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.1412, -1.7758, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.1412, -1.7758, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.6953,  0.2621,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.6953,  0.2621, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.6953,  0.2621, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.1040,  1.6253,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.1040,  1.6253, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.1040,  1.6253, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.1683, -0.3185,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.1683, -0.3185, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.1683, -0.3185, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.5214, -0.6929,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.5214, -0.6929, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.5214, -0.6929, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.6339, -1.7651,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.6339, -1.7651, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.6339, -1.7651, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.1050, -1.1935,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.1050, -1.1935, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.1050, -1.1935, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.0533, -0.6570,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.0533, -0.6570, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.0533, -0.6570, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.0340, -0.1116,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.0340, -0.1116, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.0340, -0.1116, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.0775,  1.7355,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.0775,  1.7355, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.0775,  1.7355, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.0262, -1.4210,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.0262, -1.4210, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.0262, -1.4210, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.4514,  2.6019,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.4514,  2.6019, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.4514,  2.6019, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.9415, -1.0071,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.9415, -1.0071, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.9415, -1.0071, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.0998, -0.8016,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.0998, -0.8016, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.0998, -0.8016, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.8036,  0.9967,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.8036,  0.9967, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.8036,  0.9967, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.7423, -0.8127,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.7423, -0.8127, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.7423, -0.8127, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.1588, -0.3923,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.1588, -0.3923, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.1588, -0.3923, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.3911,  1.5850,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.3911,  1.5850, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.3911,  1.5850, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.9572,  0.7582,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.9572,  0.7582, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.9572,  0.7582, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.1345,  0.0530,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.1345,  0.0530, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.1345,  0.0530, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.9236, -0.6452,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.9236, -0.6452, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.9236, -0.6452, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.2826, -0.4850,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.2826, -0.4850, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.2826, -0.4850, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.5741, -0.3762,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.5741, -0.3762, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.5741, -0.3762, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.2109, -0.3228,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.2109, -0.3228, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.2109, -0.3228, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.8779,  1.3616,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.8779,  1.3616, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.8779,  1.3616, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.2043,  0.1682,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.2043,  0.1682, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.2043,  0.1682, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.5326,  1.1254,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.5326,  1.1254, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.5326,  1.1254, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.2873,  0.1069,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.2873,  0.1069, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.2873,  0.1069, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.4564,  0.9957,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.4564,  0.9957, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.4564,  0.9957, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.9848,  1.3457,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.9848,  1.3457, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.9848,  1.3457, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.1591, -0.8784,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.1591, -0.8784, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.1591, -0.8784, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.2186,  0.9829,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.2186,  0.9829, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.2186,  0.9829, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.2869,  0.0966,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.2869,  0.0966, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.2869,  0.0966, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.0717,  0.7439,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.0717,  0.7439, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.0717,  0.7439, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-2.2338, -0.3575,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-2.2338, -0.3575, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-2.2338, -0.3575, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.1314, -0.4942,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.1314, -0.4942, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.1314, -0.4942, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.5487,  2.0403,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.5487,  2.0403, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.5487,  2.0403, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.1950, -0.5257,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.1950, -0.5257, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.1950, -0.5257, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.7858,  0.8378,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.7858,  0.8378, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.7858,  0.8378, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.1542, -0.6712,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.1542, -0.6712, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.1542, -0.6712, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.6626,  1.3906,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.6626,  1.3906, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.6626,  1.3906, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.0542,  0.7182,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.0542,  0.7182, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.0542,  0.7182, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.5619,  0.0935,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.5619,  0.0935, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.5619,  0.0935, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.5807,  0.0213,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.5807,  0.0213, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.5807,  0.0213, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.2706, -0.3281,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.2706, -0.3281, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.2706, -0.3281, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.3780,  1.4982,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.3780,  1.4982, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.3780,  1.4982, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.5252,  0.7674,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.5252,  0.7674, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.5252,  0.7674, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.2150, -0.7838,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.2150, -0.7838, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.2150, -0.7838, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.7887,  0.4684,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.7887,  0.4684, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.7887,  0.4684, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.1433,  0.8038,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.1433,  0.8038, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.1433,  0.8038, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.8453,  0.6444,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.8453,  0.6444, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.8453,  0.6444, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.2167, -1.6354,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.2167, -1.6354, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.2167, -1.6354, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.0070,  1.5198,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.0070,  1.5198, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.0070,  1.5198, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.3692, -1.6861,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.3692, -1.6861, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.3692, -1.6861, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.9990,  1.4240,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.9990,  1.4240, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.9990,  1.4240, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.8942, -0.4327,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.8942, -0.4327, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.8942, -0.4327, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.7435,  0.3904,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.7435,  0.3904, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.7435,  0.3904, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.5035, -0.2193,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.5035, -0.2193, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.5035, -0.2193, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.8927,  0.0351,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.8927,  0.0351, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.8927,  0.0351, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.6336, -1.8074,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.6336, -1.8074, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.6336, -1.8074, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.0663,  0.0802,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.0663,  0.0802, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.0663,  0.0802, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.9452,  0.5316,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.9452,  0.5316, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.9452,  0.5316, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.4771,  1.3937,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.4771,  1.3937, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.4771,  1.3937, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.8054, -0.2365,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.8054, -0.2365, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.8054, -0.2365, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.3272,  0.4396,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.3272,  0.4396, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.3272,  0.4396, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.2286, -0.1100,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.2286, -0.1100, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.2286, -0.1100, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.3867,  0.3769,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.3867,  0.3769, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.3867,  0.3769, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.0530, -0.2259,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.0530, -0.2259, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.0530, -0.2259, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.0115, -1.4724,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.0115, -1.4724, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.0115, -1.4724, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.6935,  1.3942,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.6935,  1.3942, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.6935,  1.3942, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.2762, -1.5413,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.2762, -1.5413, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.2762, -1.5413, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.3532, -0.2317,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.3532, -0.2317, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.3532, -0.2317, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.5126, -0.6581,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.5126, -0.6581, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.5126, -0.6581, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.2779, -0.1301,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.2779, -0.1301, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.2779, -0.1301, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 9.8365e-01,  1.6895e-03,  1.5710e-01, -2.1303e+00],\n",
      "       grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.9836,  0.0017, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.9836,  0.0017, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.2643,  1.1378,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.2643,  1.1378, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.2643,  1.1378, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.0245, -0.2651,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.0245, -0.2651, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.0245, -0.2651, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.7719, -0.2700,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.7719, -0.2700, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.7719, -0.2700, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.1153, -0.7263,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.1153, -0.7263, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.1153, -0.7263, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.7487, -0.7396,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.7487, -0.7396, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.7487, -0.7396, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.8274,  1.5536,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.8274,  1.5536, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.8274,  1.5536, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.3599, -0.6010,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.3599, -0.6010, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.3599, -0.6010, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.6150, -0.1384,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.6150, -0.1384, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.6150, -0.1384, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.4866,  0.6782,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.4866,  0.6782, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.4866,  0.6782, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.9060, -2.0975,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.9060, -2.0975, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.9060, -2.0975, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.3963, -1.1216,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.3963, -1.1216, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.3963, -1.1216, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.3539,  1.8616,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.3539,  1.8616, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.3539,  1.8616, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.7181,  0.9114,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.7181,  0.9114, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.7181,  0.9114, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.1167,  0.2155,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.1167,  0.2155, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.1167,  0.2155, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.8091, -1.3325,  0.1571, -2.1303], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.8091, -1.3325, -1.0111,  0.0232], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.8091, -1.3325, -0.7573, -0.6634], grad_fn=<UnbindBackward>)\n"
     ]
    }
   ],
   "source": [
    "net.generate_weight_codes()\n",
    "# print(net.weight_codes[1])\n",
    "\n",
    "for p in net.weight_codes[1]:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1571, -2.1303,  0.0000,  0.0000], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.0111,  0.0232,  0.0000,  0.0000], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.7573, -0.6634,  0.0000,  0.0000], grad_fn=<UnbindBackward>)\n"
     ]
    }
   ],
   "source": [
    "for p in net.bias_codes[-1]:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100])\n",
      "torch.Size([3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2756, -0.2985,  0.4375]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(torch.ones([1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Handles building the weight codes and draw samples from i\n",
    ":return:\n",
    "\"\"\"\n",
    "# Define initial distributions\n",
    "layer_sizes = [2, 100, 3]\n",
    "code_dim = 2\n",
    "code_mu = [torch.zeros([lsize, code_dim]) for lsize in layer_sizes]\n",
    "code_var = [torch.ones([lsize, code_dim]) for lsize in layer_sizes]\n",
    "\n",
    "# Sample the codes array\n",
    "codes = [code_mu[i] + torch.randn_like(code_mu[i]) * code_var[i] for i in range(len(layer_sizes))]\n",
    "\n",
    "weight_codes = []\n",
    "bias_codes = []\n",
    "\n",
    "# Loop between the layers and generate their weight codes by concatenating each units' latent var.\n",
    "# Units in the smaller layer need to be duplicated to the size of the next layer in order to perform\n",
    "# easy concatenation between their latent variables\n",
    "# for idx in range(len(layer_sizes) - 1):\n",
    "#     temp = codes[idx].unsqueeze(1).repeat(1, layer_sizes[idx + 1], 1).view([-1, code_dim])\n",
    "#     temp2 = codes[idx + 1].unsqueeze(0).repeat(layer_sizes[idx], 1, 1).view([-1, code_dim])\n",
    "#     concated = torch.cat((temp, temp2), dim=1)\n",
    "\n",
    "#     weight_codes.append(concated)\n",
    "\n",
    "#     # Generate bias codes (concatenation is just with a zeros vector)\n",
    "#     bias_codes.append(torch.cat((codes[idx + 1], torch.zeros_like(codes[idx + 1])), dim=1))\n",
    "    \n",
    "    \n",
    "for idx in range(len(layer_sizes) - 1):\n",
    "    temp = codes[idx].unsqueeze(1).repeat(1, layer_sizes[idx + 1], 1).view([-1, code_dim])\n",
    "    temp2 = codes[idx + 1].unsqueeze(0).repeat(layer_sizes[idx], 1, 1).view([-1, code_dim])\n",
    "    concated = torch.cat((temp2, temp), dim=1)\n",
    "\n",
    "    weight_codes.append(concated)\n",
    "\n",
    "    # Generate bias codes (concatenation is just with a zeros vector)\n",
    "    bias_codes.append(torch.cat((torch.zeros_like(codes[idx + 1]), codes[idx + 1]), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0663, -0.2537, -0.7671,  0.1655])\n",
      "tensor([-0.6406, -0.0191, -0.7671,  0.1655])\n",
      "tensor([-0.9620, -0.1154, -0.7671,  0.1655])\n",
      "tensor([ 0.7242,  0.0688, -0.7671,  0.1655])\n",
      "tensor([ 1.8329,  0.7140, -0.7671,  0.1655])\n",
      "tensor([ 0.6584,  0.2479, -0.7671,  0.1655])\n",
      "tensor([-1.8796, -0.4564, -0.7671,  0.1655])\n",
      "tensor([-2.0760, -0.7325, -0.7671,  0.1655])\n",
      "tensor([ 3.1749, -0.6530, -0.7671,  0.1655])\n",
      "tensor([ 0.3134, -0.9132, -0.7671,  0.1655])\n",
      "tensor([-1.2615, -1.7640, -0.7671,  0.1655])\n",
      "tensor([-0.5465, -1.1939, -0.7671,  0.1655])\n",
      "tensor([-1.0170,  0.7540, -0.7671,  0.1655])\n",
      "tensor([ 0.8924,  1.0713, -0.7671,  0.1655])\n",
      "tensor([ 1.9969, -0.8478, -0.7671,  0.1655])\n",
      "tensor([-0.6573,  0.5529, -0.7671,  0.1655])\n",
      "tensor([-0.6278,  0.2335, -0.7671,  0.1655])\n",
      "tensor([-0.4503,  0.9457, -0.7671,  0.1655])\n",
      "tensor([ 0.5426, -1.1243, -0.7671,  0.1655])\n",
      "tensor([-0.1318,  0.1106, -0.7671,  0.1655])\n",
      "tensor([-0.2178, -1.0761, -0.7671,  0.1655])\n",
      "tensor([ 0.8017,  0.1677, -0.7671,  0.1655])\n",
      "tensor([ 0.4213, -0.1875, -0.7671,  0.1655])\n",
      "tensor([-0.5320,  0.4953, -0.7671,  0.1655])\n",
      "tensor([ 0.0856, -1.2893, -0.7671,  0.1655])\n",
      "tensor([ 0.4385,  1.1490, -0.7671,  0.1655])\n",
      "tensor([-0.3264, -1.0795, -0.7671,  0.1655])\n",
      "tensor([-0.6788,  1.1672, -0.7671,  0.1655])\n",
      "tensor([ 0.3021,  0.5906, -0.7671,  0.1655])\n",
      "tensor([-0.3240, -0.1160, -0.7671,  0.1655])\n",
      "tensor([ 0.4717, -0.7723, -0.7671,  0.1655])\n",
      "tensor([ 0.3021, -0.9579, -0.7671,  0.1655])\n",
      "tensor([ 1.3985,  0.5811, -0.7671,  0.1655])\n",
      "tensor([ 0.4294, -1.3255, -0.7671,  0.1655])\n",
      "tensor([ 0.2007,  0.1821, -0.7671,  0.1655])\n",
      "tensor([-0.4596, -0.1692, -0.7671,  0.1655])\n",
      "tensor([ 0.2296, -0.6576, -0.7671,  0.1655])\n",
      "tensor([ 0.7246, -0.0666, -0.7671,  0.1655])\n",
      "tensor([ 1.0890,  0.0376, -0.7671,  0.1655])\n",
      "tensor([ 0.9927,  0.3407, -0.7671,  0.1655])\n",
      "tensor([-0.1779, -0.1331, -0.7671,  0.1655])\n",
      "tensor([-0.6049,  0.3820, -0.7671,  0.1655])\n",
      "tensor([ 0.1692, -0.3839, -0.7671,  0.1655])\n",
      "tensor([ 0.8964, -0.1607, -0.7671,  0.1655])\n",
      "tensor([-0.8824,  1.8593, -0.7671,  0.1655])\n",
      "tensor([ 0.9015, -1.4260, -0.7671,  0.1655])\n",
      "tensor([-0.1959, -0.1933, -0.7671,  0.1655])\n",
      "tensor([-0.7594, -0.1151, -0.7671,  0.1655])\n",
      "tensor([-0.5075, -0.2323, -0.7671,  0.1655])\n",
      "tensor([-0.0893,  1.5114, -0.7671,  0.1655])\n",
      "tensor([ 1.3571, -0.0742, -0.7671,  0.1655])\n",
      "tensor([-0.2519,  1.4461, -0.7671,  0.1655])\n",
      "tensor([ 0.1644, -0.4693, -0.7671,  0.1655])\n",
      "tensor([ 0.8991,  0.9671, -0.7671,  0.1655])\n",
      "tensor([-1.7515, -0.0388, -0.7671,  0.1655])\n",
      "tensor([-0.7311,  0.0152, -0.7671,  0.1655])\n",
      "tensor([ 0.4154, -0.9129, -0.7671,  0.1655])\n",
      "tensor([ 0.9097,  0.4222, -0.7671,  0.1655])\n",
      "tensor([ 1.0573,  0.4182, -0.7671,  0.1655])\n",
      "tensor([-0.3927, -2.6130, -0.7671,  0.1655])\n",
      "tensor([ 0.7197, -0.1624, -0.7671,  0.1655])\n",
      "tensor([ 0.7320, -0.1865, -0.7671,  0.1655])\n",
      "tensor([-0.8182, -1.1806, -0.7671,  0.1655])\n",
      "tensor([-0.6722,  0.7422, -0.7671,  0.1655])\n",
      "tensor([-0.8289, -0.8006, -0.7671,  0.1655])\n",
      "tensor([-0.0191, -0.0317, -0.7671,  0.1655])\n",
      "tensor([-0.6845, -2.2656, -0.7671,  0.1655])\n",
      "tensor([-0.1181,  0.5441, -0.7671,  0.1655])\n",
      "tensor([ 1.1740,  0.0311, -0.7671,  0.1655])\n",
      "tensor([ 1.2154,  0.2306, -0.7671,  0.1655])\n",
      "tensor([-2.2375, -0.7182, -0.7671,  0.1655])\n",
      "tensor([-2.4440,  0.1439, -0.7671,  0.1655])\n",
      "tensor([ 2.1480, -0.2465, -0.7671,  0.1655])\n",
      "tensor([ 0.4174,  0.2145, -0.7671,  0.1655])\n",
      "tensor([-0.3284, -1.8590, -0.7671,  0.1655])\n",
      "tensor([ 1.8874, -0.0021, -0.7671,  0.1655])\n",
      "tensor([-1.3320,  0.2465, -0.7671,  0.1655])\n",
      "tensor([ 0.6485,  1.0478, -0.7671,  0.1655])\n",
      "tensor([ 1.1516,  2.1134, -0.7671,  0.1655])\n",
      "tensor([-1.0982,  1.2789, -0.7671,  0.1655])\n",
      "tensor([-0.7141,  1.7311, -0.7671,  0.1655])\n",
      "tensor([-2.1871, -1.1749, -0.7671,  0.1655])\n",
      "tensor([ 0.0137,  0.5360, -0.7671,  0.1655])\n",
      "tensor([ 0.1077,  0.8005, -0.7671,  0.1655])\n",
      "tensor([ 0.5736, -0.2398, -0.7671,  0.1655])\n",
      "tensor([ 1.6373, -1.4272, -0.7671,  0.1655])\n",
      "tensor([ 0.2813, -0.2719, -0.7671,  0.1655])\n",
      "tensor([ 0.8453, -0.5452, -0.7671,  0.1655])\n",
      "tensor([ 0.0801,  2.3371, -0.7671,  0.1655])\n",
      "tensor([-2.0984, -0.9687, -0.7671,  0.1655])\n",
      "tensor([ 0.3638, -0.7637, -0.7671,  0.1655])\n",
      "tensor([ 1.5046,  0.7164, -0.7671,  0.1655])\n",
      "tensor([-0.2634, -1.3991, -0.7671,  0.1655])\n",
      "tensor([ 1.8446,  0.5212, -0.7671,  0.1655])\n",
      "tensor([-0.6006, -1.0120, -0.7671,  0.1655])\n",
      "tensor([-0.5732,  0.4104, -0.7671,  0.1655])\n",
      "tensor([-2.9895,  0.0294, -0.7671,  0.1655])\n",
      "tensor([ 1.2818, -1.4029, -0.7671,  0.1655])\n",
      "tensor([-0.0076,  1.6947, -0.7671,  0.1655])\n",
      "tensor([-0.2794,  0.0431, -0.7671,  0.1655])\n",
      "tensor([-0.0663, -0.2537,  0.5391, -1.0170])\n",
      "tensor([-0.6406, -0.0191,  0.5391, -1.0170])\n",
      "tensor([-0.9620, -0.1154,  0.5391, -1.0170])\n",
      "tensor([ 0.7242,  0.0688,  0.5391, -1.0170])\n",
      "tensor([ 1.8329,  0.7140,  0.5391, -1.0170])\n",
      "tensor([ 0.6584,  0.2479,  0.5391, -1.0170])\n",
      "tensor([-1.8796, -0.4564,  0.5391, -1.0170])\n",
      "tensor([-2.0760, -0.7325,  0.5391, -1.0170])\n",
      "tensor([ 3.1749, -0.6530,  0.5391, -1.0170])\n",
      "tensor([ 0.3134, -0.9132,  0.5391, -1.0170])\n",
      "tensor([-1.2615, -1.7640,  0.5391, -1.0170])\n",
      "tensor([-0.5465, -1.1939,  0.5391, -1.0170])\n",
      "tensor([-1.0170,  0.7540,  0.5391, -1.0170])\n",
      "tensor([ 0.8924,  1.0713,  0.5391, -1.0170])\n",
      "tensor([ 1.9969, -0.8478,  0.5391, -1.0170])\n",
      "tensor([-0.6573,  0.5529,  0.5391, -1.0170])\n",
      "tensor([-0.6278,  0.2335,  0.5391, -1.0170])\n",
      "tensor([-0.4503,  0.9457,  0.5391, -1.0170])\n",
      "tensor([ 0.5426, -1.1243,  0.5391, -1.0170])\n",
      "tensor([-0.1318,  0.1106,  0.5391, -1.0170])\n",
      "tensor([-0.2178, -1.0761,  0.5391, -1.0170])\n",
      "tensor([ 0.8017,  0.1677,  0.5391, -1.0170])\n",
      "tensor([ 0.4213, -0.1875,  0.5391, -1.0170])\n",
      "tensor([-0.5320,  0.4953,  0.5391, -1.0170])\n",
      "tensor([ 0.0856, -1.2893,  0.5391, -1.0170])\n",
      "tensor([ 0.4385,  1.1490,  0.5391, -1.0170])\n",
      "tensor([-0.3264, -1.0795,  0.5391, -1.0170])\n",
      "tensor([-0.6788,  1.1672,  0.5391, -1.0170])\n",
      "tensor([ 0.3021,  0.5906,  0.5391, -1.0170])\n",
      "tensor([-0.3240, -0.1160,  0.5391, -1.0170])\n",
      "tensor([ 0.4717, -0.7723,  0.5391, -1.0170])\n",
      "tensor([ 0.3021, -0.9579,  0.5391, -1.0170])\n",
      "tensor([ 1.3985,  0.5811,  0.5391, -1.0170])\n",
      "tensor([ 0.4294, -1.3255,  0.5391, -1.0170])\n",
      "tensor([ 0.2007,  0.1821,  0.5391, -1.0170])\n",
      "tensor([-0.4596, -0.1692,  0.5391, -1.0170])\n",
      "tensor([ 0.2296, -0.6576,  0.5391, -1.0170])\n",
      "tensor([ 0.7246, -0.0666,  0.5391, -1.0170])\n",
      "tensor([ 1.0890,  0.0376,  0.5391, -1.0170])\n",
      "tensor([ 0.9927,  0.3407,  0.5391, -1.0170])\n",
      "tensor([-0.1779, -0.1331,  0.5391, -1.0170])\n",
      "tensor([-0.6049,  0.3820,  0.5391, -1.0170])\n",
      "tensor([ 0.1692, -0.3839,  0.5391, -1.0170])\n",
      "tensor([ 0.8964, -0.1607,  0.5391, -1.0170])\n",
      "tensor([-0.8824,  1.8593,  0.5391, -1.0170])\n",
      "tensor([ 0.9015, -1.4260,  0.5391, -1.0170])\n",
      "tensor([-0.1959, -0.1933,  0.5391, -1.0170])\n",
      "tensor([-0.7594, -0.1151,  0.5391, -1.0170])\n",
      "tensor([-0.5075, -0.2323,  0.5391, -1.0170])\n",
      "tensor([-0.0893,  1.5114,  0.5391, -1.0170])\n",
      "tensor([ 1.3571, -0.0742,  0.5391, -1.0170])\n",
      "tensor([-0.2519,  1.4461,  0.5391, -1.0170])\n",
      "tensor([ 0.1644, -0.4693,  0.5391, -1.0170])\n",
      "tensor([ 0.8991,  0.9671,  0.5391, -1.0170])\n",
      "tensor([-1.7515, -0.0388,  0.5391, -1.0170])\n",
      "tensor([-0.7311,  0.0152,  0.5391, -1.0170])\n",
      "tensor([ 0.4154, -0.9129,  0.5391, -1.0170])\n",
      "tensor([ 0.9097,  0.4222,  0.5391, -1.0170])\n",
      "tensor([ 1.0573,  0.4182,  0.5391, -1.0170])\n",
      "tensor([-0.3927, -2.6130,  0.5391, -1.0170])\n",
      "tensor([ 0.7197, -0.1624,  0.5391, -1.0170])\n",
      "tensor([ 0.7320, -0.1865,  0.5391, -1.0170])\n",
      "tensor([-0.8182, -1.1806,  0.5391, -1.0170])\n",
      "tensor([-0.6722,  0.7422,  0.5391, -1.0170])\n",
      "tensor([-0.8289, -0.8006,  0.5391, -1.0170])\n",
      "tensor([-0.0191, -0.0317,  0.5391, -1.0170])\n",
      "tensor([-0.6845, -2.2656,  0.5391, -1.0170])\n",
      "tensor([-0.1181,  0.5441,  0.5391, -1.0170])\n",
      "tensor([ 1.1740,  0.0311,  0.5391, -1.0170])\n",
      "tensor([ 1.2154,  0.2306,  0.5391, -1.0170])\n",
      "tensor([-2.2375, -0.7182,  0.5391, -1.0170])\n",
      "tensor([-2.4440,  0.1439,  0.5391, -1.0170])\n",
      "tensor([ 2.1480, -0.2465,  0.5391, -1.0170])\n",
      "tensor([ 0.4174,  0.2145,  0.5391, -1.0170])\n",
      "tensor([-0.3284, -1.8590,  0.5391, -1.0170])\n",
      "tensor([ 1.8874, -0.0021,  0.5391, -1.0170])\n",
      "tensor([-1.3320,  0.2465,  0.5391, -1.0170])\n",
      "tensor([ 0.6485,  1.0478,  0.5391, -1.0170])\n",
      "tensor([ 1.1516,  2.1134,  0.5391, -1.0170])\n",
      "tensor([-1.0982,  1.2789,  0.5391, -1.0170])\n",
      "tensor([-0.7141,  1.7311,  0.5391, -1.0170])\n",
      "tensor([-2.1871, -1.1749,  0.5391, -1.0170])\n",
      "tensor([ 0.0137,  0.5360,  0.5391, -1.0170])\n",
      "tensor([ 0.1077,  0.8005,  0.5391, -1.0170])\n",
      "tensor([ 0.5736, -0.2398,  0.5391, -1.0170])\n",
      "tensor([ 1.6373, -1.4272,  0.5391, -1.0170])\n",
      "tensor([ 0.2813, -0.2719,  0.5391, -1.0170])\n",
      "tensor([ 0.8453, -0.5452,  0.5391, -1.0170])\n",
      "tensor([ 0.0801,  2.3371,  0.5391, -1.0170])\n",
      "tensor([-2.0984, -0.9687,  0.5391, -1.0170])\n",
      "tensor([ 0.3638, -0.7637,  0.5391, -1.0170])\n",
      "tensor([ 1.5046,  0.7164,  0.5391, -1.0170])\n",
      "tensor([-0.2634, -1.3991,  0.5391, -1.0170])\n",
      "tensor([ 1.8446,  0.5212,  0.5391, -1.0170])\n",
      "tensor([-0.6006, -1.0120,  0.5391, -1.0170])\n",
      "tensor([-0.5732,  0.4104,  0.5391, -1.0170])\n",
      "tensor([-2.9895,  0.0294,  0.5391, -1.0170])\n",
      "tensor([ 1.2818, -1.4029,  0.5391, -1.0170])\n",
      "tensor([-0.0076,  1.6947,  0.5391, -1.0170])\n",
      "tensor([-0.2794,  0.0431,  0.5391, -1.0170])\n"
     ]
    }
   ],
   "source": [
    "for p in weight_codes[0]:\n",
    "    print(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}