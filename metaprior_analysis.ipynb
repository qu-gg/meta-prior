{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@file metaprior_analysis.ipynb\n",
    "@author Ryan Missel\n",
    "\n",
    "This file is to deep dive into the code written for the MetaPrior and making sure that each component\n",
    "is functioning as it should, specifically how the codes are sampled and built at each iteration.\n",
    "\n",
    "In non-binary classification tasks, the model is having troubles converging - lending to the idea that\n",
    "perhaps the code buildup is incorrect. The classifier often first converges at linear decision boundaries\n",
    "through the classes and cannot converge nicely to a cross in four classes. Convergence for the 3 class problem\n",
    "takes many iterations.\n",
    "\"\"\"\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import entropy\n",
    "from metaprior.metautils import get_act, plot_metaspace_mean, plot_metaspace_var, plot_weight_correlations\n",
    "from torch.distributions import Normal, kl_divergence as kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaPrior(nn.Module):\n",
    "    def __init__(self, layer_sizes=[1, 100, 1], mean=False, local=False, activation='linear', hyperprior_dim=32, code_dim=2):\n",
    "        super(MetaPrior, self).__init__()\n",
    "        self.input_dim = layer_sizes[0]\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.code_dim = code_dim\n",
    "\n",
    "        self.local = local\n",
    "        self.mean = mean\n",
    "\n",
    "        self.acts = [get_act('leaky_relu') if i < len(layer_sizes) - 2 else get_act(activation)\n",
    "                         for i in range(len(layer_sizes) - 1)]\n",
    "\n",
    "        self.embedder = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 20),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(20, code_dim)\n",
    "        )\n",
    "        \n",
    "        self.weight_codes = []\n",
    "        self.bias_codes = []\n",
    "\n",
    "        # Define the initial weight code distribution parameters\n",
    "        self.code_mu = nn.ParameterList([\n",
    "            torch.nn.Parameter(\n",
    "                torch.zeros([lsize, code_dim]), #+ 0.1 * torch.randn([lsize, code_dim], requires_grad=True),\n",
    "                requires_grad=True)\n",
    "            for lsize in self.layer_sizes\n",
    "        ])\n",
    "\n",
    "        self.code_var = nn.ParameterList([\n",
    "            torch.nn.Parameter(torch.ones([lsize, code_dim]), # + 0.1 * torch.randn([lsize, code_dim], requires_grad=True),\n",
    "                               requires_grad=True)\n",
    "            for lsize in self.layer_sizes\n",
    "        ])\n",
    "\n",
    "        self.codes = [\n",
    "            torch.randn([lsize, code_dim]) for lsize in self.layer_sizes\n",
    "        ]\n",
    "\n",
    "\n",
    "        # Define the hyperprior network that generates the distribution parameters of the Weights\n",
    "        self.hyperprior = nn.Sequential(\n",
    "            nn.Linear(code_dim * 2, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, hyperprior_dim)\n",
    "        )\n",
    "\n",
    "        self.mean_net = nn.Linear(hyperprior_dim, 1)\n",
    "        self.var_net = nn.Linear(hyperprior_dim, 1)\n",
    "\n",
    "    def generate_weight_codes(self):\n",
    "        \"\"\"\n",
    "        Handles building the weight codes and draw samples from i\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Sample the codes array\n",
    "        self.codes = [\n",
    "            self.code_mu[i] + torch.randn_like(self.code_mu[i]) * self.code_var[i]\n",
    "            for i in range(len(self.layer_sizes))\n",
    "        ]\n",
    "\n",
    "        self.weight_codes = []\n",
    "        self.bias_codes = []\n",
    "\n",
    "        # Loop between the layers and generate their weight codes by concatenating each units' latent var.\n",
    "        # Units in the smaller layer need to be duplicated to the size of the next layer in order to perform\n",
    "        # easy concatenation between their latent variables\n",
    "        for idx in range(len(self.layer_sizes) - 1):\n",
    "            temp = self.codes[idx].unsqueeze(1).repeat(1, self.layer_sizes[idx + 1], 1).view([-1, self.code_dim])\n",
    "            temp2 = self.codes[idx + 1].unsqueeze(0).repeat(self.layer_sizes[idx], 1, 1).view([-1, self.code_dim])\n",
    "            concated = torch.cat((temp, temp2), dim=1)\n",
    "\n",
    "            self.weight_codes.append(concated)\n",
    "\n",
    "            # Generate bias codes (concatenation is just with a zeros vector)\n",
    "            self.bias_codes.append(torch.cat((self.codes[idx + 1], torch.zeros_like(self.codes[idx + 1])), dim=1))\n",
    "\n",
    "    def kl_z_term(self):\n",
    "        \"\"\"\n",
    "        KL term related to the distribution parameters of the meta-variables, with prior N(0, 1)\n",
    "        :return: Sum of the KL values over each latent variable\n",
    "        \"\"\"\n",
    "        mus = torch.cat([cmu.view([-1]) for cmu in self.code_mu])\n",
    "        var = torch.cat([cvar.view([-1]) for cvar in self.code_var])\n",
    "\n",
    "        q = Normal(mus, var)\n",
    "        N = Normal(torch.zeros(len(mus), device=mus.device), torch.ones(len(mus), device=mus.device))\n",
    "        klz = kl(q, N).sum()\n",
    "        return klz\n",
    "\n",
    "    def forward(self, x, perturb=False):\n",
    "        \"\"\"\n",
    "        Handles iterating through each layer, generating the distribution parameters and sampling\n",
    "        the weights and biases for that layer\n",
    "        :param x: input x\n",
    "        :param perturb: whether to perturb one meta-var\n",
    "        \"\"\"\n",
    "        # Generate weight codes given current latent codes\n",
    "        self.generate_weight_codes()\n",
    "        local_code = self.embedder(x)\n",
    "\n",
    "        # Perturb one meta-var to test function draws\n",
    "        if perturb:\n",
    "            indice = np.random.randint(0, self.weight_codes[0].shape[0], 1)\n",
    "            self.weight_codes[0][indice] += torch.hstack((torch.zeros([2]), 2 * torch.randn([2])))\n",
    "\n",
    "        # Iterate over layers to get output\n",
    "        for lidx in range(len(self.layer_sizes) - 1):\n",
    "            # Get latent codes of hyperprior\n",
    "            if self.local:\n",
    "                latent_w = self.hyperprior(torch.cat((self.weight_codes[lidx], local_code), dim=1))\n",
    "                latent_b = self.hyperprior(torch.cat((self.bias_codes[lidx], local_code), dim=1))\n",
    "            else:\n",
    "                latent_w = self.hyperprior(self.weight_codes[lidx])\n",
    "                latent_b = self.hyperprior(self.bias_codes[lidx])\n",
    "\n",
    "            # Sample weights\n",
    "            w_mu, w_var = self.mean_net(latent_w), self.var_net(latent_w)\n",
    "            w = (w_mu + torch.randn_like(w_mu) * w_var).view([self.layer_sizes[lidx], self.layer_sizes[lidx + 1]])\n",
    "\n",
    "            # Sample biases\n",
    "            b_mu, b_var = self.mean_net(latent_b), self.var_net(latent_b)\n",
    "            b = (b_mu + torch.randn_like(b_mu) * b_var).squeeze()\n",
    "\n",
    "            # Perform the linear layer and activate\n",
    "            x = func.linear(x, w.T, b)\n",
    "            x = self.acts[lidx](x)\n",
    "            \n",
    "        return x\n",
    "\n",
    "    def bce_predict(self, x):\n",
    "        \"\"\" Simply turns the softmax outputs into full class predictions \"\"\"\n",
    "        # Apply softmax to output.\n",
    "        pred = self.forward(x)\n",
    "\n",
    "        ans = []\n",
    "        # Pick the class with maximum weight\n",
    "        for t in pred:\n",
    "            if t < 0.5:\n",
    "                ans.append(0)\n",
    "            else:\n",
    "                ans.append(1)\n",
    "        return ans\n",
    "\n",
    "    def ce_predict(self, x):\n",
    "        \"\"\" Get class predictions from prediction vector \"\"\"\n",
    "        pred = self.forward(x).detach().cpu().numpy()\n",
    "        argmax = np.argmax(pred, axis=1)\n",
    "        return argmax\n",
    "\n",
    "    def weight_correlations(self, x, indice, layer, shift):\n",
    "        \"\"\" Get the weight vectors of two random nodes to another \"\"\"\n",
    "        # Clamp the weight code to its distribution mean\n",
    "        # self.codes = [self.code_mu[i] for i in range(len(self.layer_sizes))]\n",
    "        self.codes = [\n",
    "            self.code_mu[i] + torch.randn_like(self.code_mu[i]) * self.code_var[i]\n",
    "            for i in range(len(self.layer_sizes))\n",
    "        ]\n",
    "\n",
    "        self.codes[layer + 1][indice] = self.code_mu[layer + 1][indice] + torch.Tensor(shift)\n",
    "\n",
    "        # Generate weight codes\n",
    "        self.weight_codes = []\n",
    "        self.bias_codes = []\n",
    "\n",
    "        # Loop between the layers and generate their weight codes by concatenating each units' latent var.\n",
    "        # Units in the smaller layer need to be duplicated to the size of the next layer in order to perform\n",
    "        # easy concatenation between their latent variables\n",
    "        for idx in range(len(self.layer_sizes) - 1):\n",
    "            temp = self.codes[idx].unsqueeze(1).repeat(1, self.layer_sizes[idx + 1], 1).view([-1, self.code_dim])\n",
    "            temp2 = self.codes[idx + 1].unsqueeze(0).repeat(self.layer_sizes[idx], 1, 1).view([-1, self.code_dim])\n",
    "            concated = torch.cat((temp, temp2), dim=1)\n",
    "\n",
    "            self.weight_codes.append(concated)\n",
    "\n",
    "            # Generate bias codes (concatenation is just with a zeros vector)\n",
    "            self.bias_codes.append(torch.cat((self.codes[idx + 1], torch.zeros_like(self.codes[idx + 1])), dim=1))\n",
    "\n",
    "        # Iterate over layers to get output\n",
    "        w_out = None\n",
    "        for lidx in range(len(self.layer_sizes) - 1):\n",
    "            # Get latent codes of hyperprior\n",
    "            latent_w = self.hyperprior(self.weight_codes[lidx])\n",
    "            latent_b = self.hyperprior(self.bias_codes[lidx])\n",
    "\n",
    "            # Sample weights\n",
    "            w_mu, w_var = self.mean_net(latent_w), self.var_net(latent_w)\n",
    "            w = (w_mu + torch.randn_like(w_mu) * w_var.exp()).view([self.layer_sizes[lidx], self.layer_sizes[lidx + 1]])\n",
    "\n",
    "            # Get the sampled weight for the specific layer\n",
    "            if lidx == layer:\n",
    "                w_out = w\n",
    "\n",
    "            # Sample biases\n",
    "            b_mu, b_var = self.mean_net(latent_b), self.var_net(latent_b)\n",
    "            b = (b_mu + torch.randn_like(b_mu) * b_var.exp()).squeeze()\n",
    "\n",
    "            # Perform the linear layer and activate\n",
    "            x = func.linear(x, w.T, b)\n",
    "            x = self.acts[lidx](x)\n",
    "\n",
    "        # Return only the node weights that are cared about\n",
    "        return x, w_out[:, indice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MetaPrior(layer_sizes=[1, 100, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.3884, -0.5718, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.3884, -0.5718, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.3884, -0.5718,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.4302,  0.0160, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.4302,  0.0160, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.4302,  0.0160,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.1901, -0.4486, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.1901, -0.4486, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.1901, -0.4486,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.8306,  0.4466, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.8306,  0.4466, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.8306,  0.4466,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.5195, -0.0507, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.5195, -0.0507, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.5195, -0.0507,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.1023, -0.8338, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.1023, -0.8338, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.1023, -0.8338,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.0955, -1.6597, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.0955, -1.6597, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.0955, -1.6597,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.4075, -0.6980, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.4075, -0.6980, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.4075, -0.6980,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.2774, -1.3529, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.2774, -1.3529, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.2774, -1.3529,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.0498,  1.7384, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.0498,  1.7384, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.0498,  1.7384,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.5979, -1.0316, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.5979, -1.0316, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.5979, -1.0316,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.1504,  0.4140, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.1504,  0.4140, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.1504,  0.4140,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.1413,  0.5400, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.1413,  0.5400, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.1413,  0.5400,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.6231, -0.1109, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.6231, -0.1109, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.6231, -0.1109,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.1963, -0.9112, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.1963, -0.9112, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.1963, -0.9112,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.2679,  0.5345, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.2679,  0.5345, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.2679,  0.5345,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.8346, -0.2861, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.8346, -0.2861, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.8346, -0.2861,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 2.1325, -0.2117, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 2.1325, -0.2117, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 2.1325, -0.2117,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.1479,  0.2585, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.1479,  0.2585, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.1479,  0.2585,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.0035, -0.7862, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.0035, -0.7862, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.0035, -0.7862,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.2783, -1.9080, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.2783, -1.9080, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.2783, -1.9080,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.4229,  0.6323, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.4229,  0.6323, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.4229,  0.6323,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.6151, -0.6934, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.6151, -0.6934, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.6151, -0.6934,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.6510,  1.3791, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.6510,  1.3791, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.6510,  1.3791,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.0794, -1.6008, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.0794, -1.6008, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.0794, -1.6008,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.0788, -1.3445, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.0788, -1.3445, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.0788, -1.3445,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.4776, -0.2350, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.4776, -0.2350, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.4776, -0.2350,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.4597, -0.8707, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.4597, -0.8707, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.4597, -0.8707,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.0780,  0.8995, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.0780,  0.8995, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.0780,  0.8995,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.1653, -0.2263, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.1653, -0.2263, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.1653, -0.2263,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.9317, -1.5768, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.9317, -1.5768, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.9317, -1.5768,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.3273,  0.7367, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.3273,  0.7367, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.3273,  0.7367,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.2100,  1.7403, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.2100,  1.7403, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.2100,  1.7403,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.0490, -1.9805, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.0490, -1.9805, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.0490, -1.9805,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.1687, -1.0921, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.1687, -1.0921, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.1687, -1.0921,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.0114, -1.1019, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.0114, -1.1019, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.0114, -1.1019,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.1605,  0.4039, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.1605,  0.4039, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.1605,  0.4039,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.1484, -0.1617, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.1484, -0.1617, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.1484, -0.1617,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.6223, -1.7273, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.6223, -1.7273, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.6223, -1.7273,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.9888,  0.6191, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.9888,  0.6191, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.9888,  0.6191,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.5477, -0.1796, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.5477, -0.1796, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.5477, -0.1796,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.4493, -0.3990, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.4493, -0.3990, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.4493, -0.3990,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.0988, -1.1169, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.0988, -1.1169, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.0988, -1.1169,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.3234,  0.7336, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.3234,  0.7336, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.3234,  0.7336,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.4601,  0.0897, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.4601,  0.0897, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.4601,  0.0897,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.7978, -0.0983, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.7978, -0.0983, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.7978, -0.0983,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.3335,  0.8245, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.3335,  0.8245, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.3335,  0.8245,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.3404,  0.5313, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.3404,  0.5313, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.3404,  0.5313,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.4066, -0.5566, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.4066, -0.5566, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.4066, -0.5566,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.1921, -1.7813, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.1921, -1.7813, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.1921, -1.7813,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.5234, -0.4580, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.5234, -0.4580, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.5234, -0.4580,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.1430, -1.1995, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.1430, -1.1995, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.1430, -1.1995,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.5337, -0.3676, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.5337, -0.3676, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.5337, -0.3676,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.7940,  0.4998, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.7940,  0.4998, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.7940,  0.4998,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.6910, -0.0152, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.6910, -0.0152, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.6910, -0.0152,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.0233, -0.4118, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.0233, -0.4118, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.0233, -0.4118,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.5547,  0.0086, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.5547,  0.0086, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.5547,  0.0086,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.0506,  2.2059, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.0506,  2.2059, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.0506,  2.2059,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.1412, -0.1174, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.1412, -0.1174, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.1412, -0.1174,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.2810,  0.3382, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.2810,  0.3382, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.2810,  0.3382,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.6041,  1.1584, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.6041,  1.1584, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.6041,  1.1584,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.4241, -1.2880, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.4241, -1.2880, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.4241, -1.2880,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.4878, -1.9916, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.4878, -1.9916, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.4878, -1.9916,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.7083,  0.9826, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.7083,  0.9826, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.7083,  0.9826,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.7488,  1.8007, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.7488,  1.8007, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.7488,  1.8007,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.2480, -1.6814, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.2480, -1.6814, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.2480, -1.6814,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.7525,  1.0535, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.7525,  1.0535, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.7525,  1.0535,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.3777, -0.7843, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.3777, -0.7843, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.3777, -0.7843,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.0766, -0.6525, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.0766, -0.6525, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.0766, -0.6525,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.5173, -0.0119, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.5173, -0.0119, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.5173, -0.0119,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.4742, -0.7399, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.4742, -0.7399, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.4742, -0.7399,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.0094, -1.1931, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.0094, -1.1931, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.0094, -1.1931,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.3465,  2.4634, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.3465,  2.4634, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.3465,  2.4634,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.6319,  0.0417, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.6319,  0.0417, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.6319,  0.0417,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.8656,  0.0848, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.8656,  0.0848, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.8656,  0.0848,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.1159,  0.0474, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.1159,  0.0474, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.1159,  0.0474,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.4574,  0.5559, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.4574,  0.5559, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.4574,  0.5559,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.2456,  0.2926, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.2456,  0.2926, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.2456,  0.2926,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.3268, -0.4702, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.3268, -0.4702, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.3268, -0.4702,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.8302, -1.3860, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.8302, -1.3860, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.8302, -1.3860,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.4895, -0.2403, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.4895, -0.2403, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.4895, -0.2403,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.7898, -0.6594, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.7898, -0.6594, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.7898, -0.6594,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.5410, -0.2411, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.5410, -0.2411, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.5410, -0.2411,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.8371,  1.3316, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.8371,  1.3316, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.8371,  1.3316,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.3819, -0.6633, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.3819, -0.6633, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.3819, -0.6633,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.4927,  0.4157, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.4927,  0.4157, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.4927,  0.4157,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.3092,  0.0182, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.3092,  0.0182, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.3092,  0.0182,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.9714, -0.2044, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.9714, -0.2044, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.9714, -0.2044,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.0640, -0.0329, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.0640, -0.0329, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.0640, -0.0329,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.2829,  0.2036, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.2829,  0.2036, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.2829,  0.2036,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.2043,  0.0860, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.2043,  0.0860, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.2043,  0.0860,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.5693,  0.2079, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.5693,  0.2079, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.5693,  0.2079,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.1970,  1.7773, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.1970,  1.7773, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.1970,  1.7773,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.1373, -0.8292, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.1373, -0.8292, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.1373, -0.8292,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.8319, -0.1249, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.8319, -0.1249, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.8319, -0.1249,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.7205,  0.5265, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.7205,  0.5265, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.7205,  0.5265,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.4804, -0.6124, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.4804, -0.6124, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 1.4804, -0.6124,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.0660, -0.4303, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.0660, -0.4303, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-1.0660, -0.4303,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.0958,  1.5816, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.0958,  1.5816, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.0958,  1.5816,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.1601,  0.6441, -0.6217,  1.7311], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.1601,  0.6441, -0.2133, -0.3903], grad_fn=<UnbindBackward>)\n",
      "tensor([ 0.1601,  0.6441,  2.9750, -0.6791], grad_fn=<UnbindBackward>)\n"
     ]
    }
   ],
   "source": [
    "net.generate_weight_codes()\n",
    "# print(net.weight_codes[1])\n",
    "\n",
    "for p in net.weight_codes[1]:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6217,  1.7311,  0.0000,  0.0000], grad_fn=<UnbindBackward>)\n",
      "tensor([-0.2133, -0.3903,  0.0000,  0.0000], grad_fn=<UnbindBackward>)\n",
      "tensor([ 2.9750, -0.6791,  0.0000,  0.0000], grad_fn=<UnbindBackward>)\n"
     ]
    }
   ],
   "source": [
    "for p in net.bias_codes[-1]:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Handles building the weight codes and draw samples from i\n",
    ":return:\n",
    "\"\"\"\n",
    "# Define initial distributions\n",
    "layer_sizes = [2, 100, 3]\n",
    "code_dim = 2\n",
    "code_mu = [torch.zeros([lsize, code_dim]) for lsize in layer_sizes]\n",
    "code_var = [torch.ones([lsize, code_dim]) for lsize in layer_sizes]\n",
    "\n",
    "# Sample the codes array\n",
    "codes = [code_mu[i] + torch.randn_like(code_mu[i]) * code_var[i] for i in range(len(layer_sizes))]\n",
    "\n",
    "weight_codes = []\n",
    "bias_codes = []\n",
    "\n",
    "# Loop between the layers and generate their weight codes by concatenating each units' latent var.\n",
    "# Units in the smaller layer need to be duplicated to the size of the next layer in order to perform\n",
    "# easy concatenation between their latent variables\n",
    "# for idx in range(len(layer_sizes) - 1):\n",
    "#     temp = codes[idx].unsqueeze(1).repeat(1, layer_sizes[idx + 1], 1).view([-1, code_dim])\n",
    "#     temp2 = codes[idx + 1].unsqueeze(0).repeat(layer_sizes[idx], 1, 1).view([-1, code_dim])\n",
    "#     concated = torch.cat((temp, temp2), dim=1)\n",
    "\n",
    "#     weight_codes.append(concated)\n",
    "\n",
    "#     # Generate bias codes (concatenation is just with a zeros vector)\n",
    "#     bias_codes.append(torch.cat((codes[idx + 1], torch.zeros_like(codes[idx + 1])), dim=1))\n",
    "    \n",
    "    \n",
    "for idx in range(len(layer_sizes) - 1):\n",
    "    temp = codes[idx].unsqueeze(1).repeat(1, layer_sizes[idx + 1], 1).view([-1, code_dim])\n",
    "    temp2 = codes[idx + 1].unsqueeze(0).repeat(layer_sizes[idx], 1, 1).view([-1, code_dim])\n",
    "    concated = torch.cat((temp2, temp), dim=1)\n",
    "\n",
    "    weight_codes.append(concated)\n",
    "\n",
    "    # Generate bias codes (concatenation is just with a zeros vector)\n",
    "    bias_codes.append(torch.cat((torch.zeros_like(codes[idx + 1]), codes[idx + 1]), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.6190,  0.0557, -2.7507,  1.2097])\n",
      "tensor([-0.3273, -1.2928, -2.7507,  1.2097])\n",
      "tensor([-0.1604,  0.1112, -2.7507,  1.2097])\n",
      "tensor([-0.6080, -0.7631, -2.7507,  1.2097])\n",
      "tensor([-1.1110,  0.6588, -2.7507,  1.2097])\n",
      "tensor([-0.8877,  1.3068, -2.7507,  1.2097])\n",
      "tensor([-2.7106, -2.0531, -2.7507,  1.2097])\n",
      "tensor([ 0.8343,  0.6821, -2.7507,  1.2097])\n",
      "tensor([-1.2697, -0.3515, -2.7507,  1.2097])\n",
      "tensor([-0.0925,  0.0214, -2.7507,  1.2097])\n",
      "tensor([-0.0405, -0.7963, -2.7507,  1.2097])\n",
      "tensor([-1.8382,  0.4193, -2.7507,  1.2097])\n",
      "tensor([-0.3724, -1.0349, -2.7507,  1.2097])\n",
      "tensor([ 0.3239,  0.4600, -2.7507,  1.2097])\n",
      "tensor([ 0.0571, -0.2020, -2.7507,  1.2097])\n",
      "tensor([ 0.0103,  0.5091, -2.7507,  1.2097])\n",
      "tensor([-0.9231,  0.5619, -2.7507,  1.2097])\n",
      "tensor([-0.0176, -1.7578, -2.7507,  1.2097])\n",
      "tensor([ 0.5695, -0.5468, -2.7507,  1.2097])\n",
      "tensor([ 0.6417, -1.0731, -2.7507,  1.2097])\n",
      "tensor([ 1.2620, -0.3106, -2.7507,  1.2097])\n",
      "tensor([-0.5430, -0.0192, -2.7507,  1.2097])\n",
      "tensor([ 1.7435,  1.1507, -2.7507,  1.2097])\n",
      "tensor([-1.5292, -1.4388, -2.7507,  1.2097])\n",
      "tensor([-1.5759, -1.5638, -2.7507,  1.2097])\n",
      "tensor([ 0.0414,  0.2936, -2.7507,  1.2097])\n",
      "tensor([-0.2250,  1.5067, -2.7507,  1.2097])\n",
      "tensor([ 2.0144, -0.0418, -2.7507,  1.2097])\n",
      "tensor([ 2.6788, -1.4701, -2.7507,  1.2097])\n",
      "tensor([-1.0673,  1.6362, -2.7507,  1.2097])\n",
      "tensor([ 0.8247, -1.1317, -2.7507,  1.2097])\n",
      "tensor([ 0.1126,  0.9982, -2.7507,  1.2097])\n",
      "tensor([ 0.5514, -1.6893, -2.7507,  1.2097])\n",
      "tensor([-0.2689,  0.5263, -2.7507,  1.2097])\n",
      "tensor([ 1.2081, -0.2339, -2.7507,  1.2097])\n",
      "tensor([-0.3109,  1.2926, -2.7507,  1.2097])\n",
      "tensor([ 1.1022,  0.1851, -2.7507,  1.2097])\n",
      "tensor([ 0.2025,  1.4225, -2.7507,  1.2097])\n",
      "tensor([ 0.1085,  0.7654, -2.7507,  1.2097])\n",
      "tensor([ 0.2308, -0.1121, -2.7507,  1.2097])\n",
      "tensor([ 1.6285,  0.8596, -2.7507,  1.2097])\n",
      "tensor([ 1.7194,  0.3690, -2.7507,  1.2097])\n",
      "tensor([-0.8712,  0.1234, -2.7507,  1.2097])\n",
      "tensor([ 0.7150, -0.9735, -2.7507,  1.2097])\n",
      "tensor([ 1.3302, -0.3594, -2.7507,  1.2097])\n",
      "tensor([ 0.9173,  1.4988, -2.7507,  1.2097])\n",
      "tensor([ 0.8431,  0.0488, -2.7507,  1.2097])\n",
      "tensor([ 0.2803,  1.2142, -2.7507,  1.2097])\n",
      "tensor([-0.6121, -0.1504, -2.7507,  1.2097])\n",
      "tensor([-0.4036,  0.5661, -2.7507,  1.2097])\n",
      "tensor([-1.2472, -0.3017, -2.7507,  1.2097])\n",
      "tensor([ 0.1022, -2.5249, -2.7507,  1.2097])\n",
      "tensor([ 1.3442,  0.1727, -2.7507,  1.2097])\n",
      "tensor([ 0.1075,  0.9735, -2.7507,  1.2097])\n",
      "tensor([ 1.2995,  0.6809, -2.7507,  1.2097])\n",
      "tensor([-2.5484, -0.4860, -2.7507,  1.2097])\n",
      "tensor([ 0.6797,  1.0405, -2.7507,  1.2097])\n",
      "tensor([ 0.3851,  2.5908, -2.7507,  1.2097])\n",
      "tensor([ 0.0537, -0.5359, -2.7507,  1.2097])\n",
      "tensor([-0.3989,  0.2281, -2.7507,  1.2097])\n",
      "tensor([-1.0532,  0.7144, -2.7507,  1.2097])\n",
      "tensor([ 0.3681, -0.4879, -2.7507,  1.2097])\n",
      "tensor([ 0.2083, -0.5293, -2.7507,  1.2097])\n",
      "tensor([ 1.0751,  1.0084, -2.7507,  1.2097])\n",
      "tensor([ 0.3211, -0.2676, -2.7507,  1.2097])\n",
      "tensor([ 0.5612, -0.3305, -2.7507,  1.2097])\n",
      "tensor([-0.9017,  0.3856, -2.7507,  1.2097])\n",
      "tensor([-1.4570,  0.8355, -2.7507,  1.2097])\n",
      "tensor([-0.1176,  0.3037, -2.7507,  1.2097])\n",
      "tensor([-1.1841,  0.3252, -2.7507,  1.2097])\n",
      "tensor([-1.9145,  1.0412, -2.7507,  1.2097])\n",
      "tensor([-0.4616, -1.3736, -2.7507,  1.2097])\n",
      "tensor([ 0.6184,  0.5188, -2.7507,  1.2097])\n",
      "tensor([-0.1722,  0.7679, -2.7507,  1.2097])\n",
      "tensor([ 0.6431,  0.7037, -2.7507,  1.2097])\n",
      "tensor([-0.7656,  0.1789, -2.7507,  1.2097])\n",
      "tensor([-0.2230, -0.4209, -2.7507,  1.2097])\n",
      "tensor([ 0.1685, -0.1985, -2.7507,  1.2097])\n",
      "tensor([-0.0974, -0.7542, -2.7507,  1.2097])\n",
      "tensor([-0.9938, -0.8779, -2.7507,  1.2097])\n",
      "tensor([ 1.9924, -1.2236, -2.7507,  1.2097])\n",
      "tensor([-1.1387,  2.2211, -2.7507,  1.2097])\n",
      "tensor([-1.1323,  0.1662, -2.7507,  1.2097])\n",
      "tensor([ 1.2796, -0.7804, -2.7507,  1.2097])\n",
      "tensor([ 0.7820, -0.6548, -2.7507,  1.2097])\n",
      "tensor([-1.5046,  0.9437, -2.7507,  1.2097])\n",
      "tensor([ 0.2800, -0.5053, -2.7507,  1.2097])\n",
      "tensor([-0.4773, -1.4869, -2.7507,  1.2097])\n",
      "tensor([-1.8302, -0.3950, -2.7507,  1.2097])\n",
      "tensor([-0.2497,  1.2827, -2.7507,  1.2097])\n",
      "tensor([-1.3612,  1.2869, -2.7507,  1.2097])\n",
      "tensor([ 1.6395, -0.8641, -2.7507,  1.2097])\n",
      "tensor([-0.6820,  0.2590, -2.7507,  1.2097])\n",
      "tensor([-0.0589, -1.9628, -2.7507,  1.2097])\n",
      "tensor([-2.1842,  0.2787, -2.7507,  1.2097])\n",
      "tensor([ 0.8991,  0.7359, -2.7507,  1.2097])\n",
      "tensor([-0.2010,  0.3718, -2.7507,  1.2097])\n",
      "tensor([-0.7839, -1.3592, -2.7507,  1.2097])\n",
      "tensor([-0.6788, -0.0201, -2.7507,  1.2097])\n",
      "tensor([-0.6136, -0.6405, -2.7507,  1.2097])\n",
      "tensor([ 0.6190,  0.0557,  0.4301, -0.9811])\n",
      "tensor([-0.3273, -1.2928,  0.4301, -0.9811])\n",
      "tensor([-0.1604,  0.1112,  0.4301, -0.9811])\n",
      "tensor([-0.6080, -0.7631,  0.4301, -0.9811])\n",
      "tensor([-1.1110,  0.6588,  0.4301, -0.9811])\n",
      "tensor([-0.8877,  1.3068,  0.4301, -0.9811])\n",
      "tensor([-2.7106, -2.0531,  0.4301, -0.9811])\n",
      "tensor([ 0.8343,  0.6821,  0.4301, -0.9811])\n",
      "tensor([-1.2697, -0.3515,  0.4301, -0.9811])\n",
      "tensor([-0.0925,  0.0214,  0.4301, -0.9811])\n",
      "tensor([-0.0405, -0.7963,  0.4301, -0.9811])\n",
      "tensor([-1.8382,  0.4193,  0.4301, -0.9811])\n",
      "tensor([-0.3724, -1.0349,  0.4301, -0.9811])\n",
      "tensor([ 0.3239,  0.4600,  0.4301, -0.9811])\n",
      "tensor([ 0.0571, -0.2020,  0.4301, -0.9811])\n",
      "tensor([ 0.0103,  0.5091,  0.4301, -0.9811])\n",
      "tensor([-0.9231,  0.5619,  0.4301, -0.9811])\n",
      "tensor([-0.0176, -1.7578,  0.4301, -0.9811])\n",
      "tensor([ 0.5695, -0.5468,  0.4301, -0.9811])\n",
      "tensor([ 0.6417, -1.0731,  0.4301, -0.9811])\n",
      "tensor([ 1.2620, -0.3106,  0.4301, -0.9811])\n",
      "tensor([-0.5430, -0.0192,  0.4301, -0.9811])\n",
      "tensor([ 1.7435,  1.1507,  0.4301, -0.9811])\n",
      "tensor([-1.5292, -1.4388,  0.4301, -0.9811])\n",
      "tensor([-1.5759, -1.5638,  0.4301, -0.9811])\n",
      "tensor([ 0.0414,  0.2936,  0.4301, -0.9811])\n",
      "tensor([-0.2250,  1.5067,  0.4301, -0.9811])\n",
      "tensor([ 2.0144, -0.0418,  0.4301, -0.9811])\n",
      "tensor([ 2.6788, -1.4701,  0.4301, -0.9811])\n",
      "tensor([-1.0673,  1.6362,  0.4301, -0.9811])\n",
      "tensor([ 0.8247, -1.1317,  0.4301, -0.9811])\n",
      "tensor([ 0.1126,  0.9982,  0.4301, -0.9811])\n",
      "tensor([ 0.5514, -1.6893,  0.4301, -0.9811])\n",
      "tensor([-0.2689,  0.5263,  0.4301, -0.9811])\n",
      "tensor([ 1.2081, -0.2339,  0.4301, -0.9811])\n",
      "tensor([-0.3109,  1.2926,  0.4301, -0.9811])\n",
      "tensor([ 1.1022,  0.1851,  0.4301, -0.9811])\n",
      "tensor([ 0.2025,  1.4225,  0.4301, -0.9811])\n",
      "tensor([ 0.1085,  0.7654,  0.4301, -0.9811])\n",
      "tensor([ 0.2308, -0.1121,  0.4301, -0.9811])\n",
      "tensor([ 1.6285,  0.8596,  0.4301, -0.9811])\n",
      "tensor([ 1.7194,  0.3690,  0.4301, -0.9811])\n",
      "tensor([-0.8712,  0.1234,  0.4301, -0.9811])\n",
      "tensor([ 0.7150, -0.9735,  0.4301, -0.9811])\n",
      "tensor([ 1.3302, -0.3594,  0.4301, -0.9811])\n",
      "tensor([ 0.9173,  1.4988,  0.4301, -0.9811])\n",
      "tensor([ 0.8431,  0.0488,  0.4301, -0.9811])\n",
      "tensor([ 0.2803,  1.2142,  0.4301, -0.9811])\n",
      "tensor([-0.6121, -0.1504,  0.4301, -0.9811])\n",
      "tensor([-0.4036,  0.5661,  0.4301, -0.9811])\n",
      "tensor([-1.2472, -0.3017,  0.4301, -0.9811])\n",
      "tensor([ 0.1022, -2.5249,  0.4301, -0.9811])\n",
      "tensor([ 1.3442,  0.1727,  0.4301, -0.9811])\n",
      "tensor([ 0.1075,  0.9735,  0.4301, -0.9811])\n",
      "tensor([ 1.2995,  0.6809,  0.4301, -0.9811])\n",
      "tensor([-2.5484, -0.4860,  0.4301, -0.9811])\n",
      "tensor([ 0.6797,  1.0405,  0.4301, -0.9811])\n",
      "tensor([ 0.3851,  2.5908,  0.4301, -0.9811])\n",
      "tensor([ 0.0537, -0.5359,  0.4301, -0.9811])\n",
      "tensor([-0.3989,  0.2281,  0.4301, -0.9811])\n",
      "tensor([-1.0532,  0.7144,  0.4301, -0.9811])\n",
      "tensor([ 0.3681, -0.4879,  0.4301, -0.9811])\n",
      "tensor([ 0.2083, -0.5293,  0.4301, -0.9811])\n",
      "tensor([ 1.0751,  1.0084,  0.4301, -0.9811])\n",
      "tensor([ 0.3211, -0.2676,  0.4301, -0.9811])\n",
      "tensor([ 0.5612, -0.3305,  0.4301, -0.9811])\n",
      "tensor([-0.9017,  0.3856,  0.4301, -0.9811])\n",
      "tensor([-1.4570,  0.8355,  0.4301, -0.9811])\n",
      "tensor([-0.1176,  0.3037,  0.4301, -0.9811])\n",
      "tensor([-1.1841,  0.3252,  0.4301, -0.9811])\n",
      "tensor([-1.9145,  1.0412,  0.4301, -0.9811])\n",
      "tensor([-0.4616, -1.3736,  0.4301, -0.9811])\n",
      "tensor([ 0.6184,  0.5188,  0.4301, -0.9811])\n",
      "tensor([-0.1722,  0.7679,  0.4301, -0.9811])\n",
      "tensor([ 0.6431,  0.7037,  0.4301, -0.9811])\n",
      "tensor([-0.7656,  0.1789,  0.4301, -0.9811])\n",
      "tensor([-0.2230, -0.4209,  0.4301, -0.9811])\n",
      "tensor([ 0.1685, -0.1985,  0.4301, -0.9811])\n",
      "tensor([-0.0974, -0.7542,  0.4301, -0.9811])\n",
      "tensor([-0.9938, -0.8779,  0.4301, -0.9811])\n",
      "tensor([ 1.9924, -1.2236,  0.4301, -0.9811])\n",
      "tensor([-1.1387,  2.2211,  0.4301, -0.9811])\n",
      "tensor([-1.1323,  0.1662,  0.4301, -0.9811])\n",
      "tensor([ 1.2796, -0.7804,  0.4301, -0.9811])\n",
      "tensor([ 0.7820, -0.6548,  0.4301, -0.9811])\n",
      "tensor([-1.5046,  0.9437,  0.4301, -0.9811])\n",
      "tensor([ 0.2800, -0.5053,  0.4301, -0.9811])\n",
      "tensor([-0.4773, -1.4869,  0.4301, -0.9811])\n",
      "tensor([-1.8302, -0.3950,  0.4301, -0.9811])\n",
      "tensor([-0.2497,  1.2827,  0.4301, -0.9811])\n",
      "tensor([-1.3612,  1.2869,  0.4301, -0.9811])\n",
      "tensor([ 1.6395, -0.8641,  0.4301, -0.9811])\n",
      "tensor([-0.6820,  0.2590,  0.4301, -0.9811])\n",
      "tensor([-0.0589, -1.9628,  0.4301, -0.9811])\n",
      "tensor([-2.1842,  0.2787,  0.4301, -0.9811])\n",
      "tensor([ 0.8991,  0.7359,  0.4301, -0.9811])\n",
      "tensor([-0.2010,  0.3718,  0.4301, -0.9811])\n",
      "tensor([-0.7839, -1.3592,  0.4301, -0.9811])\n",
      "tensor([-0.6788, -0.0201,  0.4301, -0.9811])\n",
      "tensor([-0.6136, -0.6405,  0.4301, -0.9811])\n"
     ]
    }
   ],
   "source": [
    "for p in weight_codes[0]:\n",
    "    print(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}